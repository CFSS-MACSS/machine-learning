---
title: "03-tune-better-models"
format: html
---

```{r setup, include=FALSE}
options(scipen = 999)
library(tidyverse)
library(tidymodels)

# update rcis if the package is not up-to-date - only way to access bechdel
# remotes::install_github("cis-ds/rcis")
library(rcis)

bechdel <- bechdel %>%
  # remove column - unique to each row, non-informative
  select(-title)

# data splitting
set.seed(100) # Important!
bechdel_split  <- initial_split(bechdel, strata = test, prop = .9)
bechdel_train  <- training(bechdel_split)
bechdel_test   <- testing(bechdel_split)

# data resampling
set.seed(100)
bechdel_folds <- vfold_cv(bechdel_train, v = 10, strata = test)
```

## Your Turn 1

Fill in the blanks to return the accuracy and ROC AUC for this model using 10-fold cross-validation.

```{r}
tree_mod <- decision_tree(engine = "rpart") %>%
  set_mode("classification")

tree_wf <- workflow() %>%
  add_formula(test ~ .) %>%
  add_model(tree_mod)
```

Fill in the blanks to return the accuracy and ROC AUC for this model using 10-fold cross-validation.

```{r}
set.seed(100)
______ %>% 
  ______(resamples = bechdel_folds) %>% 
  ______
```

Answer:

```{r}
set.seed(100)
tree_wf %>%
  fit_resamples(resamples = bechdel_folds) %>%
  collect_metrics()
```

## Your Turn 2

Create a new parsnip model called `rf_mod`, which will learn an ensemble of classification trees from our training data using the **ranger** package. Update your `tree_wf` with this new model.

Fit your workflow with 10-fold cross-validation and compare the ROC AUC of the random forest to your single decision tree model --- which predicts the test set better?

*Hint: you'll need https://www.tidymodels.org/find/parsnip/*

```{r}
# model
rf_mod <- _____ %>% 
  _____("ranger") %>% 
  _____("classification")

# workflow
rf_wf <-
  tree_wf %>% 
  update_model(_____)

# fit with cross-validation
set.seed(100)
_____ %>% 
  fit_resamples(resamples = bechdel_folds) %>% 
  collect_metrics()
```

Answer:

```{r}
# model
rf_mod <- rand_forest(engine = "ranger") %>%
  set_mode("classification")

# workflow
rf_wf <- tree_wf %>%
  update_model(rf_mod)

# fit with cross-validation
set.seed(100)
rf_wf %>%
  fit_resamples(resamples = bechdel_folds) %>%
  collect_metrics()
```

## Your Turn 3

Challenge: Fit 3 more random forest models, each using 3, 5, and 8 variables at each split. Update your `rf_wf` with each new model. Which value maximizes the area under the ROC curve?

```{r}
rf3_mod <- rf_mod %>% 
  set_args(mtry = 3) 

rf5_mod <- rf_mod %>% 
  set_args(mtry = 5) 

rf8_mod <- rf_mod %>% 
  set_args(mtry = 8) 
```

Do this for each model above:

```{r}
_____ <- rf_wf %>% 
  update_model(_____)

set.seed(100)
_____ %>% 
  fit_resamples(resamples = bechdel_folds) %>% 
  collect_metrics()
```

Answer:

```{r}
# 3
rf3_wf <- rf_wf %>%
  update_model(rf3_mod)

set.seed(100)
rf3_wf %>%
  fit_resamples(resamples = bechdel_folds) %>%
  collect_metrics()

# 5
rf5_wf <- rf_wf %>%
  update_model(rf5_mod)

set.seed(100)
rf5_wf %>%
  fit_resamples(resamples = bechdel_folds) %>%
  collect_metrics()

# 8
rf8_wf <- rf_wf %>%
  update_model(rf8_mod)

set.seed(100)
rf8_wf %>%
  fit_resamples(resamples = bechdel_folds) %>%
  collect_metrics()
```

## Your Turn 4

Edit the random forest model to tune the `mtry` and `min_n` hyper-parameters; call the new model spec `rf_tuner`.

Update your workflow to use the tuned model.

Then use `tune_grid()` to find the best combination of hyper-parameters to maximize `roc_auc`; let tune set up the grid for you.

How does it compare to the average ROC AUC across folds from `fit_resamples()`?

```{r}
rf_mod <- rand_forest(engine = "ranger") %>% 
  set_mode("classification")

rf_wf <- workflow() %>% 
  add_formula(test ~ .) %>% 
  add_model(rf_mod)

set.seed(100) # Important!
rf_results <- rf_wf %>% 
  fit_resamples(resamples = bechdel_folds,
                metrics = metric_set(roc_auc),
                # change me to control_grid(verbose = TRUE) with tune_grid
                control = control_resamples(verbose = TRUE))

rf_results %>% 
  collect_metrics()
```

Answer:

```{r}
rf_tuner <- rand_forest(
    engine = "ranger",
    mtry = tune(),
    min_n = tune()
  ) %>%
  set_mode("classification")

rf_wf <- rf_wf %>%
  update_model(rf_tuner)

set.seed(100) # Important!
rf_results <- rf_wf %>%
  tune_grid(resamples = bechdel_folds,
            control = control_resamples(verbose = TRUE))
```

## Your Turn 5

Use `select_best()`, `finalize_workflow()`, and `last_fit()` to take the best combination of hyper-parameters from `rf_results` and use them to predict the test set.

How does our actual test ROC AUC compare to our cross-validated estimate?

```{r results='hide'}
bechdel_best <- rf_results %>%
  select_best(metric = "roc_auc")

last_rf_workflow <- rf_wf %>%
  finalize_workflow(bechdel_best)

last_rf_fit <- last_rf_workflow %>%
  last_fit(split = bechdel_split)

last_rf_fit %>%
  collect_metrics()
```







